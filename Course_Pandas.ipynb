{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c095e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to data manipulation with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ed0fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 1. What is Pandas ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40671613",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- a powerful data analysis and manipulation library for Python\n",
    "- a Python package providing fast, flexible, and expressive data structures designed to make working \n",
    "  with \"relational\" or \"labeled\" data both easy and intuitive.\n",
    "\n",
    "\n",
    "Aim :  \n",
    "- to be the fundamental high-level building block for doing practical, **real world** data analysis in Python. - - - to become **the most powerful and flexible open source data analysis / manipulation tool available in any language**.\n",
    "\n",
    "\n",
    "It is already well on its way toward this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb6285",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 2. Main features\n",
    "\n",
    " \n",
    "\n",
    "  - Easy handling of missing data in floating point as well as non-floating\n",
    "    point data.\n",
    "  - Size mutability: columns can be inserted and deleted from DataFrame and\n",
    "    higher dimensional objects\n",
    "  - Automatic and explicit data alignment: objects can be explicitly aligned\n",
    "    to a set of labels, or the user can simply ignore the labels and let\n",
    "    `Series`, `DataFrame`, etc. automatically align the data for you in\n",
    "    computations.\n",
    "  - Powerful, flexible group by functionality to perform split-apply-combine\n",
    "    operations on data sets, for both aggregating and transforming data\n",
    "  - Make it easy to convert ragged, differently-indexed data in other Python\n",
    "    and NumPy data structures into DataFrame objects.\n",
    "  - Intelligent label-based slicing, fancy indexing, and subsetting of large\n",
    "    data sets.\n",
    "  - Intuitive merging and joining data sets.\n",
    "  - Flexible reshaping and pivoting of data sets.\n",
    "  - Hierarchical labeling of axes (possible to have multiple labels per tick).\n",
    "  - Robust IO tools for loading data from flat files (CSV and delimited),\n",
    "    Excel files, databases, and saving/loading data from the ultrafast HDF5\n",
    "    format.\n",
    "  - Time series-specific functionality: date range generation and frequency\n",
    "    conversion, moving window statistics, date shifting and lagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815e706",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 3. Installing and Importing Pandas\n",
    "\n",
    "- Details on pandas installation can be found in the Pandas documentation(https://pandas.pydata.org/).  \n",
    "- For Anaconda stack users,Pandas is already installed. Once Pandas is installed, it can be imported and its version checked using these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736af7e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "pandas.__version__"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88d90170",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Just as we generally import NumPy under the alias np, we will import Pandas under the alias pd. This import convention will be used throughout the remainder of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09bbb1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f75c698",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To more explore the Pandas's built-in documentation and other relevant information, you can display it using this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874dbc26",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    " #pd?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a7ede8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# I. Introducing Pandas Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168fd93b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " The 3 fundamental Pandas data structures: \n",
    " \n",
    "        - the Series, \n",
    "        - DataFrame, \n",
    "        - and Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99083c3",
   "metadata": {},
   "source": [
    "Let us start first this section with the standard **NumPy** and **Pandas** imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82486622",
   "metadata": {},
   "source": [
    "## I.1 The Pandas Series Object\n",
    "\n",
    "A Pandas Series is a one-dimensional array of indexed data. It can be created from a list or array as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f844c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.Series([0.2, 0.35, 0.85, 1.0])\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2aab9853",
   "metadata": {},
   "source": [
    "As you can see in the output, the Series wraps both a sequence of values and a sequence of indices, which we can access with the values and index attributes. To get the values, we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5edd93f7",
   "metadata": {},
   "source": [
    "These values are simply a familiar NumPy array. For indexes we can use: The index is an array-like object of type pd.Index, which we'll discuss in more detail momentarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267bd51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "347809e7",
   "metadata": {},
   "source": [
    "Like with a NumPy array, data can be accessed by the associated index via the familiar Python square-bracket notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a380d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d5ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f7261",
   "metadata": {},
   "source": [
    " ## I.1.1 `Series` as generalized NumPy array\n",
    " \n",
    " However, the Pandas `Series` is much more general and flexible than the one-dimensional NumPy array. \n",
    "The essential difference is the presence of the index:\n",
    "\n",
    "while the Numpy Array has an implicitly defined integer index used to access the values, the Pandas Series has an explicitly defined index associated with the values.\n",
    "\n",
    "This explicit index definition gives the Series object additional capabilities. For example, the index need not be an integer, but can consist of values of any desired type. For example, if we wish, we can use strings as an index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63fc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.Series([0.2, 0.35, 0.85, 1.0], index=['a','b','c','d'])\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe0215",
   "metadata": {},
   "source": [
    "And the item access works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['c']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f3860",
   "metadata": {},
   "source": [
    "We can even use non-contiguous or non-sequential indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.Series([0.2, 0.35, 0.85, 1.0], index=[5,45,2,100])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b390b93",
   "metadata": {},
   "source": [
    "## I.1.2 `Series` as specialized dictionary.\n",
    "This  can be made even more clear by constructing a `Series` object directly from a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e089284",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_dict = {'Montpellier': 290053,\n",
    "                   'Paris': 2175601,\n",
    "                   'Troyes': 61996,\n",
    "                   'Marseille': 868277,\n",
    "                   'Lyon': 518635}\n",
    "population = pd.Series(population_dict)\n",
    "population"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06e83751",
   "metadata": {},
   "source": [
    "From here, typical dictionary-style item access can be performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd51eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "population['Montpellier']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47930025",
   "metadata": {},
   "source": [
    "Unlike a dictionary, though, the `Series` also supports array-style operations such as slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "population['Montpellier':'Troyes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f4262",
   "metadata": {},
   "source": [
    "## I.1.3 Constructing `Series` objects\n",
    "\n",
    "We've already seen a few ways of constructing a Pandas `Series` from scratch; all of them are some version of the following:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3203a3b5",
   "metadata": {},
   "source": [
    ">>> pd.Series(data, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dfa10b",
   "metadata": {},
   "source": [
    "where **index** is an optional argument, and data can be one of many entities.\n",
    "\n",
    "For example, data can be a list or NumPy array, in which case index defaults to an integer sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([5, 10, 27, 14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b391ef9",
   "metadata": {},
   "source": [
    "Data can be a scalar, which is repeated to fill the specified index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(7, index=[111, 222, 333])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb93400",
   "metadata": {},
   "source": [
    "Data can be a dictionary, in which index defaults to the sorted dictionary keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912bbaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series({2:'a', 1:'b', 3:'c'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc1fd4b",
   "metadata": {},
   "source": [
    "In each case, the index can be explicitly set if a different result is preferred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series({2:'a', 1:'b', 3:'c'}, index=[3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c8cba",
   "metadata": {},
   "source": [
    "# I.2 The Pandas `DataFrame` Object\n",
    "\n",
    "The next fundamental structure in Pandas is the `DataFrame`. Like the Series object discussed in the previous section, the `DataFrame` can be thought of either as a generalization of a NumPy array, or as a specialization of a Python dictionary. We'll now take a look at each of these perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a581c",
   "metadata": {},
   "source": [
    "## I.2.1 `DataFrame` as a generalized NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c617726",
   "metadata": {},
   "source": [
    "If a `Series` is an analog of a **one-dimensional array with flexible indices**, a `DataFrame` is an analog of a **two-dimensional array with both flexible row indices and flexible column names**. Just as you might think of a two-dimensional array as an **ordered sequence of aligned one-dimensional columns**, you can think of a DataFrame as a **sequence of aligned `Series` objects**. Here, by \"aligned\" we mean that they share the same index.\n",
    "\n",
    "To demonstrate this, let's first construct a new `Series` listing the area of each of the five cities discussed in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c88d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = {'Montpellier': 56.9,\n",
    "                   'Paris': 105.4,\n",
    "                   'Troyes': 13.2,\n",
    "                   'Marseille': 240.6,\n",
    "                   'Lyon': 47.9}\n",
    "area = pd.Series(area_dict)\n",
    "area\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95673ff",
   "metadata": {},
   "source": [
    "Now that we have this along with the population Series from before, we can use a dictionary to construct a single two-dimensional object containing this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.DataFrame({'population': population,\n",
    "                       'area': area})\n",
    "cities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1550bc",
   "metadata": {},
   "source": [
    "Like the `Series` object, the `DataFrame` has an index attribute that gives access to the index labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904bbce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca16c74c",
   "metadata": {},
   "source": [
    "Additionally, the `DataFrame` has a columns attribute, which is an Index object holding the column labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240800c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762fd54",
   "metadata": {},
   "source": [
    "Thus the `DataFrame` can be thought of as a **generalization of a two-dimensional NumPy array, where both the rows and columns have a generalized index for accessing the data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afb758",
   "metadata": {},
   "source": [
    "## I.2.2 `DataFrame` as specialized dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4120c5",
   "metadata": {},
   "source": [
    "Similarly, we can also think of a `DataFrame` as a specialization of a dictionary. Where a dictionary maps a key to a value, a `DataFrame` maps a column name to a `Series` of column data. For example, asking for the 'area' attribute returns the `Series` object containing the areas we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdec34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities['population']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c92e1",
   "metadata": {},
   "source": [
    "Notice the potential point of confusion here: in a two-dimesnional NumPy array, **data[0]** will return the first row. For a `DataFrame`, **data['col0']** will return the first column. Because of this, it is probably better to think about `DataFrame`s as generalized dictionaries rather than generalized arrays, though both ways of looking at the situation can be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4bb72",
   "metadata": {},
   "source": [
    "## I.2.3 Constructing ``DataFrame` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa502c2",
   "metadata": {},
   "source": [
    "A Pandas `DataFrame` can be constructed in a variety of ways. Here we'll give several examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5df2e",
   "metadata": {},
   "source": [
    "### I.2.3.1 From a single `Series` object\n",
    "\n",
    "A `DataFrame` is a collection of `Series` objects, and a single-column `DataFrame` can be constructed from a single `Series`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(population, columns=['population'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb10df",
   "metadata": {},
   "source": [
    "### I.2.3.2 From a list of dictionnaries¶\n",
    "\n",
    "Any list of dictionaries can be made into a `DataFrame`. We'll use a simple list comprehension to create some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711cf287",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [{'square': i*i, 'double': 2 * i}\n",
    "        for i in range(6)]\n",
    "pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cf4cd",
   "metadata": {},
   "source": [
    "Even if some keys in the dictionary are missing, Pandas will fill them in with **NaN** (i.e., \"not a number\") values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfaf03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{'column1': 15, 'column2': 40}, {'column2': 53, 'column3': 77}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1227f24",
   "metadata": {},
   "source": [
    "\n",
    "### I.2.3.3 From a dictionary of `Series` objects\n",
    "\n",
    "\n",
    "\n",
    "As we saw before, a `DataFrame` can be constructed from a dictionary of Series objects as well:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'population': population,\n",
    "              'area': area})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ff8f5",
   "metadata": {},
   "source": [
    "\n",
    "### I.2.3.4 From a two-dimensional NumPy array\n",
    "\n",
    "Given a two-dimensional array of data, we can create a `DataFrame` with any specified column and index names. If omitted, an integer index will be used for each:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da973c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.random.rand(5, 3),\n",
    "             columns=['col1', 'col2', 'col3'],\n",
    "             index=['row1', 'row2', 'row3', 'row4', 'row5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7396e2c0",
   "metadata": {},
   "source": [
    "\n",
    "### I.2.3.5 From a NumPy structured array\n",
    "\n",
    "\n",
    "We covered structured arrays in Structured Data: NumPy's Structured Arrays. A Pandas `DataFrame` operates much like a structured array, and can be created directly from one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAB = np.zeros(3, dtype=[('A', 'i8'), ('B', 'f8')])\n",
    "TAB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(TAB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad636bb6",
   "metadata": {},
   "source": [
    "# I.3 The Pandas `Index` Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97364e2",
   "metadata": {},
   "source": [
    "We have seen here that both the `Series` and `DataFrame` objects contain an explicit `index` that lets you reference and modify data. This `Index` object is an interesting structure in itself, and it can be thought of either as an immutable array or as an ordered set (technically a multi-set, as Index objects may contain repeated values). Those views have some interesting consequences in the operations available on `Index` objects. As a simple example, let's construct an ``Index` from a list of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = pd.Index([8,14, 15, 18, 20])\n",
    "ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f629d1a0",
   "metadata": {},
   "source": [
    "\n",
    "## I.3.1 `Index` as immutable array\n",
    "\n",
    "The `Index` in many ways operates like an array. For example, we can use standard Python indexing notation to retrieve values or slices:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1369db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdabf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1690ff4",
   "metadata": {},
   "source": [
    "Index objects also have many of the attributes familiar from NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ind.size, ind.shape, ind.ndim, ind.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29243d05",
   "metadata": {},
   "source": [
    "One difference between Index objects and NumPy arrays is that indices are immutable–that is, they cannot be modified via the normal means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8110a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c201dd",
   "metadata": {},
   "source": [
    "This immutability makes it safer to share indices between multiple DataFrames and arrays, without the potential for side effects from inadvertent index modification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb798a3c",
   "metadata": {},
   "source": [
    "\n",
    "## I.3.2 Index as ordered set\n",
    "\n",
    "Pandas objects are designed to facilitate operations such as joins across datasets, which depend on many aspects of set arithmetic. The Index object follows many of the conventions used by Python's built-in set data structure, so that unions, intersections, differences, and other combinations can be computed in a familiar way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "indA = pd.Index([1, 3, 5, 7, 9])\n",
    "indB = pd.Index([2, 3, 5, 7, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8fc76",
   "metadata": {},
   "source": [
    "## Intersection of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c352b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "indA & indB \n",
    "indA.intersection(indB) #also possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd7b9b",
   "metadata": {},
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indA | indB\n",
    "indA.union(indB) #also possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be13351",
   "metadata": {},
   "source": [
    "## Symmetric difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15adf266",
   "metadata": {},
   "outputs": [],
   "source": [
    "indA ^ indB\n",
    "#indA.difference(indB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a289f744",
   "metadata": {},
   "source": [
    "# II. Data Indexing and Selection\n",
    "\n",
    "We'll start with the simple case of the one-dimensional Series object, and then move on to the more complicated two-dimesnional DataFrame object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1e735",
   "metadata": {},
   "source": [
    "## I.1 Data Selection in Series\n",
    "\n",
    "As we saw in the previous section, a Series object acts in many ways like a one-dimensional NumPy array, and in many ways like a standard Python dictionary. If we keep these two overlapping analogies in mind, it will help us to understand the patterns of data indexing and selection in these arrays.\n",
    "\n",
    "\n",
    "### Series as dictionary\n",
    "\n",
    "Like a dictionary, the Series object provides a mapping from a collection of keys to a collection of values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d29c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ds = pd.Series([0.25, 0.5, 0.75, 1.0],\n",
    "                 index=['a', 'b', 'c', 'd'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1096579",
   "metadata": {},
   "source": [
    "We can also use dictionary-like Python expressions and methods to examine the keys/indices and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' in ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a6024e",
   "metadata": {},
   "source": [
    "Series objects can even be modified with a dictionary-like syntax. Just as you can extend a dictionary by assigning to a new key, you can extend a Series by assigning to a new index value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['e'] = 1.25\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3f4b7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This easy mutability of the objects is a convenient feature: under the hood, Pandas is making decisions about memory layout and data copying that might need to take place; the user generally does not need to worry about these issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930dc92b",
   "metadata": {},
   "source": [
    "\n",
    "# Series as one-dimensional array\n",
    "\n",
    "\n",
    "A Series builds on this dictionary-like interface and provides array-style item selection via the same basic mechanisms as NumPy arrays – that is, slices, masking, and fancy indexing. Examples of these are as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646ecd2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# slicing by explicit index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cc842",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['a':'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a4125",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# slicing by implicit integer index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c0af96",
   "metadata": {},
   "source": [
    "# masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[(ds > 0.3) & (ds < 0.8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623081c",
   "metadata": {},
   "source": [
    "# fancy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[['a', 'e']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf406ba3",
   "metadata": {},
   "source": [
    "Among these, slicing may be the source of the most confusion. Notice that when slicing with an explicit index (i.e., `ds['a':'c']`), the final index is included in the slice, while when slicing with an implicit index (i.e., `ds[0:2]`), the final index is excluded from the slice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469fdbc5",
   "metadata": {},
   "source": [
    "# Indexers: `loc` and `iloc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6d68e",
   "metadata": {},
   "source": [
    "These slicing and indexing conventions can be a source of confusion. For example, if your `Series` has an explicit integer index, an indexing operation such as `ds[1]` will use the explicit indices, while a slicing operation like `ds[1:3]` will use the implicit Python-style index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56819adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbdb742",
   "metadata": {},
   "source": [
    "## explicit index when indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e35da",
   "metadata": {},
   "source": [
    "## implicit index when slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727625a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab04d65",
   "metadata": {},
   "source": [
    "Because of this potential confusion in the case of integer indexes, Pandas provides some special indexer attributes that explicitly expose certain indexing schemes. These are not functional methods, but attributes that expose a particular slicing interface to the data in the Series.\n",
    "\n",
    "First, the loc attribute allows indexing and slicing that always references the explicit index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b496e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.loc[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b0ea6",
   "metadata": {},
   "source": [
    "The `iloc` attribute allows indexing and slicing that always references the implicit Python-style index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef8cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.iloc[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b91e8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "One guiding principle of Python code is that \"explicit is better than implicit.\" The explicit nature of loc and iloc make them very useful in maintaining clean and readable code; especially in the case of integer indexes, I recommend using these both to make code easier to read and understand, and to prevent subtle bugs due to the mixed indexing/slicing convention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df95d4f",
   "metadata": {},
   "source": [
    "# II.2 Data Selection in DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b893c1",
   "metadata": {},
   "source": [
    "## DataFrame as a dictionary¶\n",
    "\n",
    "The first analogy we will consider is the `DataFrame` as a dictionary of related `Series` objects. Let's return to our example of areas and populations of states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "area = {'Montpellier': 56.9,\n",
    "                   'Paris': 105.4,\n",
    "                   'Troyes': 13.2,\n",
    "                   'Marseille': 240.6,\n",
    "                   'Lyon': 47.9}\n",
    "\n",
    "population = {'Montpellier': 290053,\n",
    "                   'Paris': 2175601,\n",
    "                   'Troyes': 61996,\n",
    "                   'Marseille': 868277,\n",
    "                   'Lyon': 518635}\n",
    "cities_data = pd.DataFrame({'population': population,\n",
    "              'area': area})\n",
    "cities_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d110ab",
   "metadata": {},
   "source": [
    "The individual Series that make up the columns of the DataFrame can be accessed via dictionary-style indexing of the column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9700fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data['area']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61543b0e",
   "metadata": {},
   "source": [
    "Equivalently, we can use attribute-style access with column names that are strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data.area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77993b9",
   "metadata": {},
   "source": [
    "This attribute-style column access actually accesses the exact same object as the dictionary-style access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data.area is cities_data['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24791b05",
   "metadata": {},
   "source": [
    "Though this is a useful shorthand, keep in mind that it does not work for all cases! For example, if the column names are not strings, or if the column names conflict with methods of the `DataFrame`, this attribute-style access is not possible. For example, the `DataFrame` has a `population()` method, so `data.pop` will point to this rather than the \"population\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aef31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data.population is cities_data['population']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e02c28",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In particular, you should avoid the temptation to try column assignment via attribute (i.e., use `cities_data['population'] = z` rather than `cities_data.population = z`).\n",
    "\n",
    "Like with the `Series` objects discussed earlier, this dictionary-style syntax can also be used to modify the object, in this case adding a new column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed33fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data['density'] = cities_data['population'] / cities_data['area']\n",
    "cities_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af84b0",
   "metadata": {},
   "source": [
    "This shows a preview of the straightforward syntax of element-by-element arithmetic between Series objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c6d5f",
   "metadata": {},
   "source": [
    "\n",
    "## DataFrame as two-dimensional array\n",
    "\n",
    "As mentioned previously, we can also view the `DataFrame` as an enhanced two-dimensional array. We can examine the raw underlying data array using the `values` attribute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6734b",
   "metadata": {},
   "source": [
    "With this picture in mind, many familiar array-like observations can be done on the `DataFrame` itself. For example, we can transpose the full `DataFrame` to swap rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2413f",
   "metadata": {},
   "source": [
    "When it comes to indexing of `DataFrame` objects, however, it is clear that the dictionary-style indexing of columns precludes our ability to simply treat it as a NumPy array. In particular, passing a single index to an array accesses a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b1221d",
   "metadata": {},
   "source": [
    "and passing a single \"index\" to a `DataFrame` accesses a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bfc826",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f3a4a",
   "metadata": {},
   "source": [
    "Thus for array-style indexing, we need another convention. Here Pandas again uses the `loc` and `iloc` mentioned earlier. Using the `iloc` indexer, we can index the underlying array as if it is a simple NumPy array (using the implicit Python-style index), but the `DataFrame` index and column labels are maintained in the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data.iloc[:3, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f611345",
   "metadata": {},
   "source": [
    "Similarly, using the `loc` indexer we can index the underlying data in an array-like style but using the explicit index and column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data.loc[:'Troyes', :'population']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60761ace",
   "metadata": {},
   "source": [
    "Keep in mind that for integer indices, the `ix` indexer is subject to the same potential sources of confusion as discussed for integer-indexed `Series` objects.\n",
    "\n",
    "Any of the familiar NumPy-style data access patterns can be used within these indexers. For example, in the `loc` indexer we can combine masking and fancy indexing as in the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5605b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data.loc[cities_data.density > 5000, ['population', 'density']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781bb57c",
   "metadata": {},
   "source": [
    "Any of these indexing conventions may also be used to set or modify values; this is done in the standard way that you might be accustomed to from working with NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafcbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data.iloc[0, 2] = 10000\n",
    "cities_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5be7df",
   "metadata": {},
   "source": [
    "To build up your fluency in Pandas data manipulation, I suggest spending some time with a simple `DataFrame` and exploring the types of indexing, slicing, masking, and fancy indexing that are allowed by these various indexing approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a455e2",
   "metadata": {},
   "source": [
    "\n",
    "## Additional indexing conventions\n",
    "\n",
    "There are a couple extra indexing conventions that might seem at odds with the preceding discussion, but nevertheless can be very useful in practice. First, while `indexing` refers to columns, `slicing` refers to rows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38469551",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data['Paris':'Troyes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c03d1",
   "metadata": {},
   "source": [
    "Such slices can also refer to rows by number rather than by index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51071e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60b0e1",
   "metadata": {},
   "source": [
    "Similarly, direct masking operations are also interpreted row-wise rather than column-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data[cities_data.density > 5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21758d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "These two conventions are syntactically similar to those on a NumPy array, and while these may not precisely fit the mold of the Pandas conventions, they are nevertheless quite useful in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "ser = pd.Series(rng.randint(0, 10, 4))\n",
    "ser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca63c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rng.randint(0, 10, (3, 4)),\n",
    "                  columns=['A', 'B', 'C', 'D'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01898255",
   "metadata": {},
   "source": [
    "If we apply a NumPy ufunc on either of these objects, the result will be another Pandas object with the indices preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(ser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16caec80",
   "metadata": {},
   "source": [
    "Or, for a slightly more complex calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sin(df * np.pi / 4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0409400",
   "metadata": {},
   "source": [
    "Any of the ufuncs can be used in a similar manner:\n",
    "    \n",
    "    \n",
    "Operator \tEquivalent ufunc \tDescription\n",
    " + \t        np.add \t            Addition (e.g., 1 + 1 = 2)\n",
    "\n",
    " - \t        np.subtract \t    Subtraction (e.g., 3 - 2 = 1)*\n",
    " - \t        np.negative \t    Unary negation (e.g., -2)\n",
    " * \t        np.multiply \t    Multiplication (e.g., 2 * 3 = 6)\n",
    " / \t        np.divide \t        Division (e.g., 3 / 2 = 1.5)\n",
    " // \t    np.floor_divide \tFloor division (e.g., 3 // 2 = 1)\n",
    " ** \t    np.power \t        Exponentiation (e.g., 2 ** 3 = 8)\n",
    " % \t        np.mod \t            Modulus/remainder (e.g., 9 % 4 = 1)\n",
    " ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33d866",
   "metadata": {},
   "source": [
    "# III. Operating on Data in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4730e",
   "metadata": {},
   "source": [
    "\n",
    "# UFuncs: Index Alignment\n",
    "\n",
    "For binary operations on two Series or DataFrame objects, Pandas will align indices in the process of performing the operation. This is very convenient when working with incomplete data, as we'll see in some of the examples that follow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb8020",
   "metadata": {},
   "source": [
    "## Index alignment in Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7549137",
   "metadata": {},
   "source": [
    "As an example, suppose we are combining two different data sources, and find only the top three French cities by population and the top three French cities by area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.Series({'Paris': 2175601,\n",
    "                        'Marseille': 868277,\n",
    "                        'Lyon': 518635}, name='population')\n",
    "area = pd.Series({'Marseille': 240.6,\n",
    "                   'Paris': 105.4,\n",
    "                   'Montpellier': 56.9}, name='area')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b93274",
   "metadata": {},
   "source": [
    "Let's see what happens when we divide these to compute the population density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8395f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "population / area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ad1d2",
   "metadata": {},
   "source": [
    "The resulting array contains the union of indices of the two input arrays, which could be determined using standard Python set arithmetic on these indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d78197",
   "metadata": {},
   "outputs": [],
   "source": [
    "area.index | population.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e750a",
   "metadata": {},
   "source": [
    "Any item for which one or the other does not have an entry is marked with `NaN`, or \"Not a Number,\" which is how Pandas marks missing data. This index matching is implemented this way for any of Python's built-in arithmetic expressions; any missing values are filled in with `NaN` by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a58988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_A = pd.Series([2, 4, 6], index=[0, 1, 2])\n",
    "ds_B = pd.Series([1, 3, 5], index=[1, 2, 3])\n",
    "ds_A + ds_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe3df6",
   "metadata": {},
   "source": [
    "If using `NaN` values is not the desired behavior, the fill value can be modified using appropriate object methods in place of the operators. For example, calling `ds_A.add(ds_B)` is equivalent to calling `ds_A + ds_B`, but allows optional explicit specification of the fill value for any elements in `ds_A` or `ds_B` that might be missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1144bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_A.add(ds_B, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11e34e6",
   "metadata": {},
   "source": [
    "\n",
    "## Index alignment in DataFrame\n",
    "\n",
    "A similar type of alignment takes place for both columns and indices when performing operations on `DataFrame`s:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab68f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = pd.DataFrame(rng.randint(0, 5, (2, 2)),\n",
    "                 columns=list('XY'))\n",
    "df_A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B = pd.DataFrame(rng.randint(0, 15, (3, 3)),\n",
    "                 columns=list('YXZ'))\n",
    "df_B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695472b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A+df_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414aa193",
   "metadata": {},
   "source": [
    "Notice that indices are aligned correctly irrespective of their order in the two objects, and indices in the result are sorted. As was the case with `Series`, we can use the associated object's arithmetic method and pass any desired `fill_value` to be used in place of missing entries. Here we'll fill with the mean of all values in `df_A` (computed by first stacking the rows of `df_A`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea122f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = df_A.stack().mean()\n",
    "df_A.add(df_B, fill_value=fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8dd38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9655396f",
   "metadata": {},
   "source": [
    "The following table lists Python operators and their equivalent Pandas object methods:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da0c4c18",
   "metadata": {},
   "source": [
    "Python Operator \tPandas Method(s)\n",
    "    + \t             add()\n",
    "    - \t             sub(), subtract()\n",
    "    * \t             mul(), multiply()\n",
    "    / \t             truediv(), div(), divide()\n",
    "    // \t             floordiv()\n",
    "    % \t             mod()\n",
    "    ** \t             pow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab82550",
   "metadata": {},
   "source": [
    "# Ufuncs: Operations Between DataFrame and Series\n",
    "\n",
    "When performing operations between a `DataFrame` and a `Series`, the index and column alignment is similarly maintained. Operations between a `DataFrame` and a `Series` are similar to operations between a two-dimensional and one-dimensional NumPy array. Consider one common operation, where we find the difference of a two-dimensional array and one of its rows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rng.randint(10, size=(3, 4))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ecd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "A - A[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad8fa5",
   "metadata": {},
   "source": [
    "According to NumPy's broadcasting rules (see Computation on Arrays: Broadcasting), subtraction between a two-dimensional array and one of its rows is applied row-wise.\n",
    "\n",
    "In Pandas, the convention similarly operates row-wise by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(A, columns=list('QRST'))\n",
    "df - df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d1dab",
   "metadata": {},
   "source": [
    "If you would instead like to operate column-wise, you can use the object methods mentioned earlier, while specifying the axis keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d67022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.subtract(df['R'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447e93d",
   "metadata": {},
   "source": [
    "Note that these DataFrame/Series operations, like the operations discussed above, will automatically align indices between the two elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c949c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "halfrow = df.iloc[0, ::2]\n",
    "halfrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671619f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df - halfrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e441ed6",
   "metadata": {},
   "source": [
    "This preservation and alignment of indices and columns means that operations on data in Pandas will always maintain the data context, which prevents the types of silly errors that might come up when working with heterogeneous and/or misaligned data in raw NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584b54d",
   "metadata": {},
   "source": [
    "# IV. Missing Data in Pandas¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c3f75",
   "metadata": {},
   "source": [
    "Pandas chose to use sentinels for missing data, and further chose to use two already-existing Python null values: the special floating-point `NaN` value, and the Python `None` object. This choice has some side effects, as we will see, but in practice ends up being a good compromise in most cases of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee7bdb",
   "metadata": {},
   "source": [
    "## `None`: Pythonic missing data\n",
    "\n",
    "The first sentinel value used by Pandas is `None`, a Python singleton object that is often used for missing data in Python code. Because it is a Python object, `None` cannot be used in any arbitrary NumPy/Pandas array, but only in arrays with data type `'object'` (i.e., arrays of Python objects):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1daf207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7736a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = np.array([1, None, 3, 4])\n",
    "vals1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea196d9",
   "metadata": {},
   "source": [
    "This `dtype=object` means that the best common type representation NumPy could infer for the contents of the array is that they are Python objects. While this kind of object array is useful for some purposes, any operations on the data will be done at the Python level, with much more overhead than the typically fast operations seen for arrays with native types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915464f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in ['object', 'int']:\n",
    "    print(\"dtype =\", dtype)\n",
    "    %timeit np.arange(1E6, dtype=dtype).sum()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e143c81",
   "metadata": {},
   "source": [
    "The use of Python objects in an array also means that if you perform aggregations like `sum()` or `min()` across an array with a `None` value, you will generally get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51463345",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8688e1",
   "metadata": {},
   "source": [
    "This reflects the fact that addition between an integer and `None` is undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be5abd",
   "metadata": {},
   "source": [
    "## `NaN`: Missing numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd027f60",
   "metadata": {},
   "source": [
    "The other missing data representation, `NaN` (acronym for Not a Number), is different; it is a special floating-point value recognized by all systems that use the standard IEEE floating-point representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92829d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2 = np.array([1, np.nan, 3, 4]) \n",
    "vals2.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdad16f",
   "metadata": {},
   "source": [
    "Notice that NumPy chose a native floating-point type for this array: this means that unlike the object array from before, this array supports fast operations pushed into compiled code. You should be aware that `NaN` is a bit like a data virus–it infects any other object it touches. Regardless of the operation, the result of arithmetic with `NaN` will be another `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "0 *  np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50526f33",
   "metadata": {},
   "source": [
    "Note that this means that aggregates over the values are well defined (i.e., they don't result in an error) but not always useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a508df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2.sum(), vals2.min(), vals2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cee13b",
   "metadata": {},
   "source": [
    "NumPy does provide some special aggregations that will ignore these missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f08272",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2), np.nanmean(vals2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990021c9",
   "metadata": {},
   "source": [
    "Keep in mind that `NaN` is specifically a floating-point value; there is no equivalent `NaN` value for integers, strings, or other types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b635342",
   "metadata": {},
   "source": [
    "## `NaN` and `None` in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf441da",
   "metadata": {},
   "source": [
    "`NaN` and `None` both have their place, and Pandas is built to handle the two of them nearly interchangeably, converting between them where appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ada68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([1, np.nan, 2, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f49db2",
   "metadata": {},
   "source": [
    "For types that don't have an available sentinel value, Pandas automatically type-casts when NA values are present. For example, if we set a value in an integer array to `np.nan`, it will automatically be upcast to a floating-point type to accommodate the NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc516041",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(range(2), dtype=int)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7aec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0] = None\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438dd10",
   "metadata": {},
   "source": [
    "Notice that in addition to casting the integer array to floating point, Pandas automatically converts the `None` to a `NaN` value\n",
    "\n",
    "The following table lists the upcasting conventions in Pandas when NA values are introduced:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1dfa87cf",
   "metadata": {},
   "source": [
    "Typeclass \tConversion When Storing NAs \tNA Sentinel Value\n",
    "floating \t   No change \t                   np.nan\n",
    "object \t       No change \t                   None or np.nan\n",
    "integer \t   Cast to float64 \t               np.nan\n",
    "boolean \t   Cast to object \t               None or np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5a752",
   "metadata": {},
   "source": [
    "Keep in mind that in Pandas, string data is always stored with an `object` dtype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526afcfc",
   "metadata": {},
   "source": [
    "\n",
    "# Operating on Null Values¶\n",
    "\n",
    "As we have seen, Pandas treats `None` and `NaN` as essentially interchangeable for indicating missing or null values. To facilitate this convention, there are several useful methods for detecting, removing, and replacing null values in Pandas data structures. They are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b53a3",
   "metadata": {},
   "source": [
    " - `isnull()`: Generate a boolean mask indicating missing values\n",
    " - `notnull()`: Opposite of isnull()\n",
    " - `dropna()`: Return a filtered version of the data\n",
    " -`fillna()`: Return a copy of the data with missing values filled or imputed\n",
    "\n",
    "We will conclude this section with a brief exploration and demonstration of these routines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d389695e",
   "metadata": {},
   "source": [
    "## Detecting null values¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c7053d",
   "metadata": {},
   "source": [
    "Pandas data structures have two useful methods for detecting null data: `isnull()` and `notnull()`. Either one will return a Boolean mask over the data. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 'hello', None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fe24d",
   "metadata": {},
   "source": [
    "Boolean masks can be used directly as a `Series` or `DataFrame` index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232eed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff41221",
   "metadata": {},
   "source": [
    "The `isnull()` and `notnull()` methods produce similar Boolean results for `DataFrame`s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999958c9",
   "metadata": {},
   "source": [
    "\n",
    "## Dropping null values\n",
    "\n",
    "In addition to the masking used before, there are the convenience methods, `dropna()` (which removes NA values) and `fillna()` (which fills in NA values). For a `Series`, the result is straightforward:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210b585",
   "metadata": {},
   "source": [
    "For a `DataFrame`, there are more options. Consider the following `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a14ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1,      np.nan, 2],\n",
    "                   [2,      3,      5],\n",
    "                   [np.nan, 4,      6]])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559b876",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We cannot drop single values from a `DataFrame`; we can only drop full rows or full columns. Depending on the application, you might want one or the other, so `dropna()` gives a number of options for a `DataFrame`.\n",
    "\n",
    "By default, `dropna()` will drop all rows in which any null value is present:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5125f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bfd1b0",
   "metadata": {},
   "source": [
    "Alternatively, you can drop NA values along a different axis; `axis=1` drops all columns containing a null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d526f",
   "metadata": {},
   "source": [
    "But this drops some good data as well; you might rather be interested in dropping rows or columns with *all* NA values, or a majority of NA values. This can be specified through the `how` or `thresh` parameters, which allow fine control of the number of nulls to allow through.\n",
    "\n",
    "The default is `how='any'`, such that any row or column (depending on the axis keyword) containing a null value will be dropped. You can also specify `how='all'`, which will only drop rows/columns that are *all* null values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='columns', how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c756993f",
   "metadata": {},
   "source": [
    "For finer-grained control, the `thresh` parameter lets you specify a minimum number of non-null values for the row/column to be kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='rows', thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3816b",
   "metadata": {},
   "source": [
    "Here the first and last row have been dropped, because they contain only two non-null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab82a8",
   "metadata": {},
   "source": [
    "\n",
    "## Filling null values\n",
    "\n",
    "Sometimes rather than dropping NA values, you'd rather replace them with a valid value. This value might be a single number like zero, or it might be some sort of imputation or interpolation from the good values. You could do this in-place using the `isnull()` method as a mask, but because it is such a common operation Pandas provides the `fillna()` method, which returns a copy of the array with the null values replaced.\n",
    "\n",
    "Consider the following `Series`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4defaf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e0e28",
   "metadata": {},
   "source": [
    "We can fill NA entries with a single value, such as zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c74fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d697baca",
   "metadata": {},
   "source": [
    "We can specify a forward-fill to propagate the previous value forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5afa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward-fill\n",
    "data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01b5eab",
   "metadata": {},
   "source": [
    "Or we can specify a back-fill to propagate the next values backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49487331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back-fill\n",
    "data.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243627c",
   "metadata": {},
   "source": [
    "For `DataFrame`s, the options are similar, but we can also specify an axis along which the fills take place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fda839",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06717a",
   "metadata": {},
   "source": [
    "Notice that if a previous value is not available during a forward fill, the NA value remains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b669378",
   "metadata": {},
   "source": [
    "\n",
    "# V. Hierarchical Indexing\n",
    "\n",
    "\n",
    "Up to this point we've been focused primarily on one-dimensional and two-dimensional data, stored in Pandas Series and DataFrame objects, respectively. Often it is useful to go beyond this and store higher-dimensional data–that is, data indexed by more than one or two keys. While Pandas does provide Panel and Panel4D objects that natively handle three-dimensional and four-dimensional data (see Aside: Panel Data), a far more common pattern in practice is to make use of hierarchical indexing (also known as multi-indexing) to incorporate multiple index levels within a single index. In this way, higher-dimensional data can be compactly represented within the familiar one-dimensional Series and two-dimensional DataFrame objects.\n",
    "\n",
    "In this section, we'll explore the direct creation of MultiIndex objects, considerations when indexing, slicing, and computing statistics across multiply indexed data, and useful routines for converting between simple and hierarchically indexed representations of your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c0f9a",
   "metadata": {},
   "source": [
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab536dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2b70d",
   "metadata": {},
   "source": [
    "# A Multiply Indexed Series\n",
    "\n",
    "Let's start by considering how we might represent two-dimensional data within a one-dimensional Series. For concreteness, we will consider a series of data where each point has a character and numerical key.\n",
    "\n",
    "## The bad way\n",
    "\n",
    "Suppose you would like to track data about states from two different years. Using the Pandas tools we've already covered, you might be tempted to simply use Python tuples as keys:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcaf6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [('Paris', 2013), ('Paris', 2018),\n",
    "         ('Marseille', 2013), ('Marseille', 2018),\n",
    "         ('Lyon', 2013), ('Lyon', 2018)]\n",
    "populations = [2229621, 2175601,\n",
    "               855393, 868277,\n",
    "               500715, 518635]\n",
    "cities_pop = pd.Series(populations, index=index)\n",
    "cities_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba1916",
   "metadata": {},
   "source": [
    "With this indexing scheme, you can straightforwardly index or slice the series based on this multiple index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5bec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop[('Paris', 2018):('Lyon', 2013)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159f260",
   "metadata": {},
   "source": [
    "But the convenience ends there. For example, if you need to select all values from 2018, you'll need to do some messy (and potentially slow) munging to make it happen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a48eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop[[i for i in cities_pop.index if i[1] == 2018]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6578fa",
   "metadata": {},
   "source": [
    "This produces the desired result, but is not as clean (or as efficient for large datasets) as the slicing syntax we've grown to love in Pandas.\n",
    "\n",
    "## The Better Way: Pandas MultiIndex\n",
    "\n",
    "Fortunately, Pandas provides a better way. Our tuple-based indexing is essentially a rudimentary multi-index, and the Pandas MultiIndex type gives us the type of operations we wish to have. We can create a multi-index from the tuples as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a27895",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_tuples(index)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the MultiIndex contains multiple levels of indexing–in \n",
    "#this case, the state names and the years, as well as multiple labels \n",
    "#for each data point which encode these levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b45b8",
   "metadata": {},
   "source": [
    "If we re-index our series with this MultiIndex, we see the hierarchical representation of the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258480b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop = cities_pop.reindex(index)\n",
    "cities_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2ce3c",
   "metadata": {},
   "source": [
    "Here the first two columns of the Series representation show the multiple index values, while the third column shows the data. Notice that some entries are missing in the first column: in this multi-index representation, any blank entry indicates the same value as the line above it.\n",
    "\n",
    "Now to access all data for which the second index is 2018, we can simply use the Pandas slicing notation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop[:, 2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95dc12d",
   "metadata": {},
   "source": [
    "The result is a singly indexed array with just the keys we're interested in. This syntax is much more convenient (and the operation is much more efficient!) than the home-spun tuple-based multi-indexing solution that we started with. We'll now further discuss this sort of indexing operation on hieararchically indexed data.\n",
    "\n",
    "## MultiIndex as extra dimension\n",
    "\n",
    "You might notice something else here: we could easily have stored the same data using a simple DataFrame with index and column labels. In fact, Pandas is built with this equivalence in mind. The unstack() method will quickly convert a multiply indexed Series into a conventionally indexed DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a829694",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop_df = cities_pop.unstack()\n",
    "cities_pop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d9229e",
   "metadata": {},
   "source": [
    "Naturally, the stack() method provides the opposite operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop_df.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eaf274",
   "metadata": {},
   "source": [
    "Seeing this, you might wonder why would we would bother with hierarchical indexing at all. The reason is simple: just as we were able to use multi-indexing to represent two-dimensional data within a one-dimensional Series, we can also use it to represent data of three or more dimensions in a Series or DataFrame. Each extra level in a multi-index represents an extra dimension of data; taking advantage of this property gives us much more flexibility in the types of data we can represent. Concretely, we might want to add another column of demographic data for each state at each year (say, students) ; with a MultiIndex this is as easy as adding another column to the DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop_df = pd.DataFrame({'total': cities_pop,\n",
    "                       'students': [625000 , 654455,\n",
    "                                   90000 , 92148,\n",
    "                                   140000 , 155440]})\n",
    "cities_pop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b38890",
   "metadata": {},
   "source": [
    "In addition, all the ufuncs and other functionality discussed in Operating on Data in Pandas work with hierarchical indices as well. Here we compute the fraction of students by year, given the above data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea12198",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_students = cities_pop_df['students'] / cities_pop_df['total']\n",
    "f_students.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48332902",
   "metadata": {},
   "source": [
    "This allows us to easily and quickly manipulate and explore even high-dimensional data.\n",
    "Methods of MultiIndex Creation\n",
    "\n",
    "The most straightforward way to construct a multiply indexed Series or DataFrame is to simply pass a list of two or more index arrays to the constructor. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26985c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.rand(4, 2),\n",
    "                  index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\n",
    "                  columns=['data1', 'data2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab567b",
   "metadata": {},
   "source": [
    "The work of creating the MultiIndex is done in the background.\n",
    "\n",
    "Similarly, if you pass a dictionary with appropriate tuples as keys, Pandas will automatically recognize this and use a MultiIndex by default:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {('Paris', 2013): 2229621, \n",
    "        ('Paris', 2018): 2175601,\n",
    "        ('Marseille', 2013): 855393, \n",
    "        ('Marseille', 2018): 868277,\n",
    "        ('Lyon', 2013): 500715, \n",
    "        ('Lyon', 2018): 518635}\n",
    "pd.Series(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf958b8",
   "metadata": {},
   "source": [
    "Nevertheless, it is sometimes useful to explicitly create a MultiIndex; we'll see a couple of these methods here.\n",
    "\n",
    "##  Explicit MultiIndex constructors\n",
    "\n",
    "For more flexibility in how the index is constructed, you can instead use the class method constructors available in the pd.MultiIndex. For example, as we did before, you can construct the MultiIndex from a simple list of arrays giving the index values within each level:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c120c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59b8e7",
   "metadata": {},
   "source": [
    "You can construct it from a list of tuples giving the multiple index values of each point:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d668b6ee",
   "metadata": {},
   "source": [
    "You can even construct it from a Cartesian product of single indices:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.MultiIndex.from_product([['a', 'b'], [1, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c8dda",
   "metadata": {},
   "source": [
    "Similarly, you can construct the MultiIndex directly using its internal encoding by passing levels (a list of lists containing available index values for each level) and codes (a list of lists that reference these labels):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.MultiIndex(levels=[['a', 'b'], [1, 2]],\n",
    "              codes=[[0, 0, 1, 1], [0, 1, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee8d83",
   "metadata": {},
   "source": [
    "Any of these objects can be passed as the index argument when creating a Series or Dataframe, or be passed to the reindex method of an existing Series or DataFrame.\n",
    "\n",
    "## MultiIndex level names\n",
    "\n",
    "Sometimes it is convenient to name the levels of the MultiIndex. This can be accomplished by passing the names argument to any of the above MultiIndex constructors, or by setting the names attribute of the index after the fact:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb52281",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop.index.names = ['city', 'year']\n",
    "cities_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181585d",
   "metadata": {},
   "source": [
    "With more involved datasets, this can be a useful way to keep track of the meaning of various index values.\n",
    "\n",
    "## MultiIndex for columns\n",
    "\n",
    "In a DataFrame, the rows and columns are completely symmetric, and just as the rows can have multiple levels of indices, the columns can have multiple levels as well. Consider the following, which is a mock-up of some (somewhat realistic) medical data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36fedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical indices and columns\n",
    "index = pd.MultiIndex.from_product([[2013, 2014], [1, 2]],\n",
    "                                   names=['year', 'visit'])\n",
    "columns = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'], ['HR', 'Temp']],\n",
    "                                     names=['subject', 'type'])\n",
    "\n",
    "# mock some data\n",
    "data = np.round(np.random.randn(4, 6), 1)\n",
    "data[:, ::2] *= 10\n",
    "data += 37\n",
    "\n",
    "# create the DataFrame\n",
    "health_data = pd.DataFrame(data, index=index, columns=columns)\n",
    "health_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc42d4d",
   "metadata": {},
   "source": [
    "Here we see where the multi-indexing for both rows and columns can come in very handy. This is fundamentally four-dimensional data, where the dimensions are the subject, the measurement type, the year, and the visit number. With this in place we can, for example, index the top-level column by the person's name and get a full DataFrame containing just that person's information:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5dd659",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data['Guido']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b154f2",
   "metadata": {},
   "source": [
    "For complicated records containing multiple labeled measurements across multiple times for many subjects (people, countries, cities, etc.) use of hierarchical rows and columns can be extremely convenient!\n",
    "\n",
    "# Indexing and Slicing a MultiIndex\n",
    "\n",
    "Indexing and slicing on a MultiIndex is designed to be intuitive, and it helps if you think about the indices as added dimensions. We'll first look at indexing multiply indexed Series, and then multiply-indexed DataFrames.\n",
    "\n",
    "## Multiply indexed Series\n",
    "\n",
    "Consider the multiply indexed Series of state populations we saw earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9dccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368f3f0",
   "metadata": {},
   "source": [
    "We can access single elements by indexing with multiple terms:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885cb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop['Paris', 2013]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b5a7a8",
   "metadata": {},
   "source": [
    "The MultiIndex also supports partial indexing, or indexing just one of the levels in the index. The result is another Series, with the lower-level indices maintained:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop['Paris']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe07ae",
   "metadata": {},
   "source": [
    "Partial slicing is available as well, as long as the MultiIndex is sorted (see discussion in Sorted and Unsorted Indices):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop.loc['Paris':'Lyon']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01510dd",
   "metadata": {},
   "source": [
    "With sorted indices, partial indexing can be performed on lower levels by passing an empty slice in the first index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop[:, 2013]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb2262",
   "metadata": {},
   "source": [
    "Other types of indexing and selection (discussed in Data Indexing and Selection) work as well; for example, selection based on Boolean masks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a49bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop[cities_pop > 22000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1bb815",
   "metadata": {},
   "source": [
    "Selection based on fancy indexing also works:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e23ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop[['Paris', 'Marseille']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba829a69",
   "metadata": {},
   "source": [
    "## Multiply indexed DataFrames\n",
    "\n",
    "A multiply indexed DataFrame behaves in a similar manner. Consider our toy medical DataFrame from before:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd737d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0248c66",
   "metadata": {},
   "source": [
    "Remember that columns are primary in a DataFrame, and the syntax used for multiply indexed Series applies to the columns. For example, we can recover Guido's heart rate data with a simple operation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data['Guido', 'HR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efede60e",
   "metadata": {},
   "source": [
    "Also, as with the single-index case, we can use the loc, iloc, and ix indexers introduced in Data Indexing and Selection. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5901042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data.iloc[:2, :2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a9b8d",
   "metadata": {},
   "source": [
    "These indexers provide an array-like view of the underlying two-dimensional data, but each individual index in loc or iloc can be passed a tuple of multiple indices. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data.loc[:, ('Bob', 'HR')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d5b0c",
   "metadata": {},
   "source": [
    "Working with slices within these index tuples is not especially convenient; trying to create a slice within a tuple will lead to a syntax error:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96772e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data.loc[(:, 1), (:, 'HR')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b3dcc8",
   "metadata": {},
   "source": [
    "You could get around this by building the desired slice explicitly using Python's built-in slice() function, but a better way in this context is to use an IndexSlice object, which Pandas provides for precisely this situation. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80293ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "health_data.loc[idx[:, 1], idx[:, 'HR']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16796046",
   "metadata": {},
   "source": [
    "\n",
    "There are so many ways to interact with data in multiply indexed Series and DataFrames, and as with many tools in this book the best way to become familiar with them is to try them out!\n",
    "# Rearranging Multi-Indices\n",
    "\n",
    "One of the keys to working with multiply indexed data is knowing how to effectively transform the data. There are a number of operations that will preserve all the information in the dataset, but rearrange it for the purposes of various computations. We saw a brief example of this in the stack() and unstack() methods, but there are many more ways to finely control the rearrangement of data between hierarchical indices and columns, and we'll explore them here.\n",
    "## Sorted and unsorted indices\n",
    "\n",
    "Earlier, we briefly mentioned a caveat, but we should emphasize it more here. Many of the MultiIndex slicing operations will fail if the index is not sorted. Let's take a look at this here.\n",
    "\n",
    "We'll start by creating some simple multiply indexed data where the indices are not lexographically sorted:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([['a', 'c', 'b'], [1, 2]])\n",
    "data = pd.Series(np.random.rand(6), index=index)\n",
    "data.index.names = ['char', 'int']\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949b555",
   "metadata": {},
   "source": [
    "If we try to take a partial slice of this index, it will result in an error:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be500e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data['a':'b']\n",
    "except KeyError as e:\n",
    "    print(type(e))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8389be",
   "metadata": {},
   "source": [
    "Although it is not entirely clear from the error message, this is the result of the MultiIndex not being sorted. For various reasons, partial slices and other similar operations require the levels in the MultiIndex to be in sorted (i.e., lexographical) order. Pandas provides a number of convenience routines to perform this type of sorting; examples are the sort_index() and sortlevel() methods of the DataFrame. We'll use the simplest, sort_index(), here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1080df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_index()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f6882",
   "metadata": {},
   "source": [
    "With the index sorted in this way, partial slicing will work as expected:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['a':'b']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6551d7",
   "metadata": {},
   "source": [
    "## Stacking and unstacking indices\n",
    "\n",
    "As we saw briefly before, it is possible to convert a dataset from a stacked multi-index to a simple two-dimensional representation, optionally specifying the level to use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70402121",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop.unstack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f60c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop.unstack(level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469df5a5",
   "metadata": {},
   "source": [
    "The opposite of unstack() is stack(), which here can be used to recover the original series:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop.unstack().stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffccafe",
   "metadata": {},
   "source": [
    "## Index setting and resetting\n",
    "\n",
    "Another way to rearrange hierarchical data is to turn the index labels into columns; this can be accomplished with the reset_index method. Calling this on the population dictionary will result in a DataFrame with a state and year column holding the information that was formerly in the index. For clarity, we can optionally specify the name of the data for the column representation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1351c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop_flat = cities_pop.reset_index(name='population')\n",
    "cities_pop_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed64c2",
   "metadata": {},
   "source": [
    "Often when working with data in the real world, the raw input data looks like this and it's useful to build a MultiIndex from the column values. This can be done with the set_index method of the DataFrame, which returns a multiply indexed DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_pop_flat.set_index(['city', 'year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188de4a",
   "metadata": {},
   "source": [
    "In practice, I find this type of reindexing to be one of the more useful patterns when encountering real-world datasets.\n",
    "\n",
    "# Data Aggregations on Multi-Indices\n",
    "\n",
    "We've previously seen that Pandas has built-in data aggregation methods, such as mean(), sum(), and max(). For hierarchically indexed data, these can be passed a level parameter that controls which subset of the data the aggregate is computed on.\n",
    "\n",
    "For example, let's return to our health data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63652292",
   "metadata": {},
   "source": [
    "Perhaps we'd like to average-out the measurements in the two visits each year. We can do this by naming the index level we'd like to explore, in this case the year:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = health_data.mean(level='year')\n",
    "data_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f100785",
   "metadata": {},
   "source": [
    "By further making use of the axis keyword, we can take the mean among levels on the columns as well:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean.mean(axis=1, level='type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d18f7",
   "metadata": {},
   "source": [
    "Thus in two lines, we've been able to find the average heart rate and temperature measured among all subjects in all visits each year. This syntax is actually a short cut to the GroupBy functionality, which we will discuss in Aggregation and Grouping. While this is a toy example, many real-world datasets have similar hierarchical structure.\n",
    "\n",
    "# Aside: Panel Data\n",
    "\n",
    "Pandas has a few other fundamental data structures that we have not yet discussed, namely the pd.Panel and pd.Panel4D objects. These can be thought of, respectively, as three-dimensional and four-dimensional generalizations of the (one-dimensional) Series and (two-dimensional) DataFrame structures. Once you are familiar with indexing and manipulation of data in a Series and DataFrame, Panel and Panel4D are relatively straightforward to use. In particular, the ix, loc, and iloc indexers discussed in Data Indexing and Selection extend readily to these higher-dimensional structures.\n",
    "\n",
    "We won't cover these panel structures further in this text, as I've found in the majority of cases that multi-indexing is a more useful and conceptually simpler representation for higher-dimensional data. Additionally, panel data is fundamentally a dense data representation, while multi-indexing is fundamentally a sparse data representation. As the number of dimensions increases, the dense representation can become very inefficient for the majority of real-world datasets. For the occasional specialized application, however, these structures can be useful. If you'd like to read more about the Panel and Panel4D structures, see the references listed in Further Resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce09d9d",
   "metadata": {},
   "source": [
    "# VI. Combining Datasets: Concat and Append\n",
    "\n",
    "Some of the most interesting studies of data come from combining different data sources. These operations can involve anything from very straightforward concatenation of two different datasets, to more complicated database-style joins and merges that correctly handle any overlaps between the datasets. Series and DataFrames are built with this type of operation in mind, and Pandas includes functions and methods that make this sort of data wrangling fast and straightforward.\n",
    "\n",
    "Here we'll take a look at simple concatenation of Series and DataFrames with the pd.concat function; later we'll dive into more sophisticated in-memory merges and joins implemented in Pandas.\n",
    "\n",
    "We begin with the standard imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a296fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d64db",
   "metadata": {},
   "source": [
    "For convenience, we'll define this function which creates a DataFrame of a particular form that will be useful below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be46458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(cols, ind):\n",
    "    \"\"\"Quickly make a DataFrame\"\"\"\n",
    "    data = {c: [str(c) + str(i) for i in ind]\n",
    "            for c in cols}\n",
    "    return pd.DataFrame(data, ind)\n",
    "\n",
    "# example DataFrame\n",
    "make_df('ABC', range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e3643",
   "metadata": {},
   "source": [
    "In addition, we'll create a quick class that allows us to display multiple DataFrames side by side. The code makes use of the special _repr_html_ method, which IPython uses to implement its rich object display:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617abba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
    "                         for a in self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
    "                           for a in self.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023c152",
   "metadata": {},
   "source": [
    "The use of this will become clearer as we continue our discussion in the following section.\n",
    "Recall:     \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6918993",
   "metadata": {},
   "source": [
    " ## Concatenation of NumPy Arrays\n",
    "    \n",
    "Concatenation of Series and DataFrame objects is very similar to concatenation of Numpy arrays, which can be done via the np.concatenate function as discussed in The Basics of NumPy Arrays. Recall that with it, you can combine the contents of two or more arrays into a single array:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe1bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "z = [7, 8, 9]\n",
    "np.concatenate([x, y, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a7d3b",
   "metadata": {},
   "source": [
    "The first argument is a list or tuple of arrays to concatenate. Additionally, it takes an axis keyword that allows you to specify the axis along which the result will be concatenated:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1, 2],\n",
    "     [3, 4]]\n",
    "np.concatenate([x, x], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3039674",
   "metadata": {},
   "source": [
    "# Simple Concatenation with pd.concat\n",
    "\n",
    "Pandas has a function, pd.concat(), which has a similar syntax to np.concatenate but contains a number of options that we'll discuss momentarily:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1416fdd",
   "metadata": {},
   "source": [
    "# Signature in Pandas v0.18\n",
    "pd.concat(\n",
    "    objs,\n",
    "    axis=0,\n",
    "    join=\"outer\",\n",
    "    ignore_index=False,\n",
    "    keys=None,\n",
    "    levels=None,\n",
    "    names=None,\n",
    "    verify_integrity=False,\n",
    "    copy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9441c7",
   "metadata": {},
   "source": [
    "pd.concat() can be used for a simple concatenation of Series or DataFrame objects, just as np.concatenate() can be used for simple concatenations of arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])\n",
    "ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])\n",
    "pd.concat([ser1, ser2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3173533",
   "metadata": {},
   "source": [
    "It also works to concatenate higher-dimensional objects, such as DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = make_df('ABC', [0, 1, 2, 3, 4, 5, 6, 7, 8,])\n",
    "df2 = make_df('ABC', [9, 10])\n",
    "display('df1', 'df2', 'pd.concat([df1, df2])')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072666c",
   "metadata": {},
   "source": [
    "By default, the concatenation takes place row-wise within the DataFrame (i.e., axis=0). Like np.concatenate, pd.concat allows specification of an axis along which concatenation will take place. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = make_df('AB', [0, 1])\n",
    "df4 = make_df('CD', [0, 1])\n",
    "display('df3', 'df4', \"pd.concat([df3, df4], axis='columns')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8af945",
   "metadata": {},
   "source": [
    "We could have equivalently specified axis=1; here we've used the more intuitive axis='columns'.\n",
    "## Duplicate indices\n",
    "One important difference between np.concatenate and pd.concat is that Pandas concatenation preserves indices, even if the result will have duplicate indices! Consider this simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0aaeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = make_df('AB', [0, 1])\n",
    "y = make_df('AB', [2, 3])\n",
    "y.index = x.index  # make duplicate indices!\n",
    "display('x', 'y', 'pd.concat([x, y])')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0605de",
   "metadata": {},
   "source": [
    "Notice the repeated indices in the result. While this is valid within DataFrames, the outcome is often undesirable. `pd.concat()` gives us a few ways to handle it.\n",
    "Catching the repeats as an error\n",
    "\n",
    "If you'd like to simply verify that the indices in the result of `pd.concat()` do not overlap, you can specify the verify_integrity flag. With this set to True, the concatenation will raise an exception if there are duplicate indices. Here is an example, where for clarity we'll catch and print the error message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0531e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pd.concat([x, y], verify_integrity=True)\n",
    "except ValueError as e:\n",
    "    print(\"ValueError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b67fff3",
   "metadata": {},
   "source": [
    "## Ignoring the index\n",
    "\n",
    "Sometimes the index itself does not matter, and you would prefer it to simply be ignored. This option can be specified using the ignore_index flag. With this set to true, the concatenation will create a new integer index for the resulting Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a515de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('x', 'y', 'pd.concat([x, y], ignore_index=True)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91827db5",
   "metadata": {},
   "source": [
    "## Adding MultiIndex keys\n",
    "\n",
    "Another option is to use the keys option to specify a label for the data sources; the result will be a hierarchically indexed series containing the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('x', 'y', \"pd.concat([x, y], keys=['x', 'y'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c00e82",
   "metadata": {},
   "source": [
    "The result is a multiply indexed DataFrame, and we can use the tools discussed in Hierarchical Indexing to transform this data into the representation we're interested in.\n",
    "Concatenation with joins\n",
    "\n",
    "In the simple examples we just looked at, we were mainly concatenating DataFrames with shared column names. In practice, data from different sources might have different sets of column names, and pd.concat offers several options in this case. Consider the concatenation of the following two DataFrames, which have some (but not all!) columns in common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15359cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = make_df('ABC', [1, 2])\n",
    "df6 = make_df('BCD', [3, 4])\n",
    "display('df5', 'df6', 'pd.concat([df5, df6])')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9279d8",
   "metadata": {},
   "source": [
    "By default, the entries for which no data is available are filled with NA values. To change this, we can specify one of several options for the join and join_axes parameters of the concatenate function. By default, the join is a union of the input columns (join='outer'), but we can change this to an intersection of the columns using join='inner':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c9e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df5', 'df6',\n",
    "        \"pd.concat([df5, df6], join='inner')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673d07a8",
   "metadata": {},
   "source": [
    "Another option is to directly specify the index of the remaininig colums using the join_axes argument, which takes a list of index objects. Here we'll specify that the returned columns should be the same as those of the first input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df5', 'df6',\n",
    "        \"pd.concat([df5, df6], join_axes=[df5.columns])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef12821",
   "metadata": {},
   "source": [
    "The combination of options of the pd.concat function allows a wide range of possible behaviors when joining two datasets; keep these in mind as you use these tools for your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a913b613",
   "metadata": {},
   "source": [
    "# The `append()` method\n",
    "\n",
    "Because direct array concatenation is so common, Series and DataFrame objects have an append method that can accomplish the same thing in fewer keystrokes. For example, rather than calling pd.concat([df1, df2]), you can simply call df1.append(df2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46cf591",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df1', 'df2', 'df1.append(df2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1847eac1",
   "metadata": {},
   "source": [
    "Keep in mind that unlike the append() and extend() methods of Python lists, the append() method in Pandas does not modify the original object–instead it creates a new object with the combined data. It also is not a very efficient method, because it involves creation of a new index and data buffer. Thus, if you plan to do multiple append operations, it is generally better to build a list of DataFrames and pass them all at once to the concat() function.\n",
    "\n",
    "In the next section, we'll look at another more powerful approach to combining data from multiple sources, the database-style merges/joins implemented in pd.merge. For more information on concat(), append(), and related functionality, see the \"Merge, Join, and Concatenate\" section of the Pandas documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaefa57",
   "metadata": {},
   "source": [
    "# VII Combining Datasets: Merge and Join\n",
    "ne essential feature offered by Pandas is its high-performance, in-memory join and merge operations. If you have ever worked with databases, you should be familiar with this type of data interaction. The main interface for this is the pd.merge function, and we'll see few examples of how this can work in practice.\n",
    "\n",
    "For convenience, we will start by redefining the display() functionality from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3074efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
    "                         for a in self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
    "                           for a in self.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622b0c3",
   "metadata": {},
   "source": [
    "\n",
    "# Categories of Joins¶\n",
    "\n",
    "The pd.merge() function implements a number of types of joins: the one-to-one, many-to-one, and many-to-many joins. All three types of joins are accessed via an identical call to the pd.merge() interface; the type of join performed depends on the form of the input data. Here we will show simple examples of the three types of merges, and discuss detailed options further below.\n",
    "# One-to-one joins\n",
    "\n",
    "Perhaps the simplest type of merge expresion is the one-to-one join, which is in many ways very similar to the column-wise concatenation seen in Combining Datasets: Concat & Append. As a concrete example, consider the following two DataFrames which contain information on several employees in a company:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.DataFrame({'employee': ['Bob', 'Tony', 'Thalita', 'Joe'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Tony', 'Bob', 'Thalita', 'Joe'],\n",
    "                    'hire_date': [2010, 2020, 2021, 2014]})\n",
    "display('df1', 'df2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155e2a3",
   "metadata": {},
   "source": [
    "To combine this information into a single DataFrame, we can use the pd.merge() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e78f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df1, df2)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55153bc6",
   "metadata": {},
   "source": [
    "The pd.merge() function recognizes that each DataFrame has an \"employee\" column, and automatically joins using this column as a key. The result of the merge is a new DataFrame that combines the information from the two inputs. Notice that the order of entries in each column is not necessarily maintained: in this case, the order of the \"employee\" column differs between df1 and df2, and the pd.merge() function correctly accounts for this. Additionally, keep in mind that the merge in general discards the index, except in the special case of merges by index (see the left_index and right_index keywords, discussed momentarily).\n",
    "# Many-to-one joins\n",
    "\n",
    "Many-to-one joins are joins in which one of the two key columns contains duplicate entries. For the many-to-one case, the resulting DataFrame will preserve those duplicate entries as appropriate. Consider the following example of a many-to-one join:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b36a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n",
    "                    'supervisor': ['Janet', 'James', 'Steve']})\n",
    "display('df3', 'df4', 'pd.merge(df3, df4)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f39fc3d",
   "metadata": {},
   "source": [
    "The resulting DataFrame has an aditional column with the \"supervisor\" information, where the information is repeated in one or more locations as required by the inputs.\n",
    "# Many-to-many joins\n",
    "\n",
    "Many-to-many joins are a bit confusing conceptually, but are nevertheless well defined. If the key column in both the left and right array contains duplicates, then the result is a many-to-many merge. This will be perhaps most clear with a concrete example. Consider the following, where we have a DataFrame showing one or more skills associated with a particular group. By performing a many-to-many join, we can recover the skills associated with any individual person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be87f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "                               'spreadsheets', 'organization']})\n",
    "display('df1', 'df5', \"pd.merge(df1, df5)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e199d9a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "These three types of joins can be used with other Pandas tools to implement a wide array of functionality. But in practice, datasets are rarely as clean as the one we're working with here. In the following section we'll consider some of the options provided by pd.merge() that enable you to tune how the join operations work.\n",
    "# Specification of the Merge Key\n",
    "\n",
    "We've already seen the default behavior of pd.merge(): it looks for one or more matching column names between the two inputs, and uses this as the key. However, often the column names will not match so nicely, and pd.merge() provides a variety of options for handling this.\n",
    "# The on keyword\n",
    "\n",
    "Most simply, you can explicitly specify the name of the key column using the on keyword, which takes a column name or a list of column names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb96cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df1', 'df2', \"pd.merge(df1, df2, on='employee')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91afc8a",
   "metadata": {},
   "source": [
    "This option works only if both the left and right DataFrames have the specified column name.\n",
    "# The left_on and right_on keywords\n",
    "\n",
    "At times you may wish to merge two datasets with different column names; for example, we may have a dataset in which the employee name is labeled as \"name\" rather than \"employee\". In this case, we can use the left_on and right_on keywords to specify the two column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbefd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'name': ['Bob', 'Tony', 'Thalita', 'Joe'],\n",
    "                    'salary': [70000, 80000, 120000, 90000]})\n",
    "display('df1', 'df3', 'pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcc6f4",
   "metadata": {},
   "source": [
    "The result has a redundant column that we can drop if desired–for example, by using the drop() method of DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad2071",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\").drop('name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c845c",
   "metadata": {},
   "source": [
    "\n",
    "# The left_index and right_index keywords\n",
    "\n",
    "Sometimes, rather than merging on a column, you would instead like to merge on an index. For example, your data might look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1a = df1.set_index('employee')\n",
    "df2a = df2.set_index('employee')\n",
    "display('df1a', 'df2a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e19759",
   "metadata": {},
   "source": [
    "You can use the index as the key for merging by specifying the left_index and/or right_index flags in pd.merge():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "display('df1a', 'df2a',\n",
    "        \"pd.merge(df1a, df2a, left_index=True, right_index=True)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a14863",
   "metadata": {},
   "source": [
    "For convenience, DataFrames implement the join() method, which performs a merge that defaults to joining on indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f442686",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df1a', 'df2a', 'df1a.join(df2a)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cbb9d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If you'd like to mix indices and columns, you can combine left_index with right_on or left_on with right_index to get the desired behavior:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf95eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df1a', 'df3', \"pd.merge(df1a, df3, left_index=True, right_on='name')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddea485",
   "metadata": {},
   "source": [
    "All of these options also work with multiple indices and/or multiple columns; the interface for this behavior is very intuitive. For more information on this, see the \"Merge, Join, and Concatenate\" section of the Pandas documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137b88e",
   "metadata": {},
   "source": [
    "\n",
    "# Specifying Set Arithmetic for Joins\n",
    "\n",
    "In all the preceding examples we have glossed over one important consideration in performing a join: the type of set arithmetic used in the join. This comes up when a value appears in one key column but not the other. Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415bd60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],\n",
    "                    'food': ['fish', 'beans', 'bread']},\n",
    "                   columns=['name', 'food'])\n",
    "df7 = pd.DataFrame({'name': ['Mary', 'Joseph'],\n",
    "                    'drink': ['wine', 'beer']},\n",
    "                   columns=['name', 'drink'])\n",
    "display('df6', 'df7', 'pd.merge(df6, df7)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d0f5c6",
   "metadata": {},
   "source": [
    "Here we have merged two datasets that have only a single \"name\" entry in common: Mary. By default, the result contains the intersection of the two sets of inputs; this is what is known as an inner join. We can specify this explicitly using the how keyword, which defaults to \"inner\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22878dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df6, df7, how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac18ee",
   "metadata": {},
   "source": [
    "Other options for the how keyword are 'outer', 'left', and 'right'. An outer join returns a join over the union of the input columns, and fills in all missing values with NAs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df6', 'df7', \"pd.merge(df6, df7, how='outer')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f391228",
   "metadata": {},
   "source": [
    "The left join and right join return joins over the left entries and right entries, respectively. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531fa2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df6', 'df7', \"pd.merge(df6, df7, how='left')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edce071",
   "metadata": {},
   "source": [
    "The output rows now correspond to the entries in the left input. Using how='right' works in a similar manner.\n",
    "\n",
    "All of these options can be applied straightforwardly to any of the preceding join types.\n",
    "# Overlapping Column Names: The suffixes Keyword\n",
    "\n",
    "Finally, you may end up in a case where your two input DataFrames have conflicting column names. Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df8 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'rank': [1, 2, 3, 4]})\n",
    "df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'rank': [3, 1, 4, 2]})\n",
    "display('df8', 'df9', 'pd.merge(df8, df9, on=\"name\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d83d8",
   "metadata": {},
   "source": [
    "Because the output would have two conflicting column names, the merge function automatically appends a suffix _x or _y to make the output columns unique. If these defaults are inappropriate, it is possible to specify a custom suffix using the suffixes keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c92501",
   "metadata": {},
   "outputs": [],
   "source": [
    "display('df8', 'df9', 'pd.merge(df8, df9, on=\"name\", suffixes=[\"_L\", \"_R\"])')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80506590",
   "metadata": {},
   "source": [
    "These suffixes work in any of the possible join patterns, and work also if there are multiple overlapping columns.\n",
    "\n",
    "For more information on these patterns, see Aggregation and Grouping where we dive a bit deeper into relational algebra. Also see the Pandas \"Merge, Join and Concatenate\" documentation for further discussion of these topics.\n",
    "Example: US States Data\n",
    "\n",
    "Merge and join operations come up most often when combining data from different sources. Here we will consider an example of some data about US states and their populations. The data files can be found at http://github.com/jakevdp/data-USstates/:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53594b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following are shell commands to download the data\n",
    "# !curl -O https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-population.csv\n",
    "# !curl -O https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-areas.csv\n",
    "# !curl -O https://raw.githubusercontent.com/jakevdp/data-USstates/master/state-abbrevs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b9ba4",
   "metadata": {},
   "source": [
    "Let's take a look at the three datasets, using the Pandas read_csv() function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7881875",
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.read_csv('data-USstates/state-population.csv')\n",
    "areas = pd.read_csv('data-USstates/state-areas.csv')\n",
    "abbreviations = pd.read_csv('data-USstates/state-abbrevs.csv')\n",
    "\n",
    "display('population.head()', 'areas.head()', 'abbreviations.head()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0d6aa8",
   "metadata": {},
   "source": [
    "Given this information, say we want to compute a relatively straightforward result: rank US states and territories by their 2010 population density. We clearly have the data here to find this result, but we'll have to combine the datasets to find the result.\n",
    "\n",
    "We'll start with a many-to-one merge that will give us the full state name within the population DataFrame. We want to merge based on the state/region column of pop, and the abbreviation column of abbrevs. We'll use how='outer' to make sure no data is thrown away due to mismatched labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c76ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(population, abbreviations, how='outer',\n",
    "                  left_on='state/region', right_on='abbreviation')\n",
    "merged = merged.drop('abbreviation', 1) # drop duplicate info\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f1b8a",
   "metadata": {},
   "source": [
    "Let's double-check whether there were any mismatches here, which we can do by looking for rows with nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74329bac",
   "metadata": {},
   "source": [
    "Some of the population info is null; let's figure out which these are!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged['population'].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9bc0c",
   "metadata": {},
   "source": [
    "It appears that all the null population values are from Puerto Rico prior to the year 2000; this is likely due to this data not being available from the original source.\n",
    "\n",
    "More importantly, we see also that some of the new state entries are also null, which means that there was no corresponding entry in the abbrevs key! Let's figure out which regions lack this match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c044a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.loc[merged['state'].isnull(), 'state/region'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b6d56",
   "metadata": {},
   "source": [
    "We can quickly infer the issue: our population data includes entries for Puerto Rico (PR) and the United States as a whole (USA), while these entries do not appear in the state abbreviation key. We can fix these quickly by filling in appropriate entries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bbdad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.loc[merged['state/region'] == 'PR', 'state'] = 'Puerto Rico'\n",
    "merged.loc[merged['state/region'] == 'USA', 'state'] = 'United States'\n",
    "merged.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a8dba2",
   "metadata": {},
   "source": [
    "No more nulls in the state column: we're all set!\n",
    "\n",
    "Now we can merge the result with the area data using a similar procedure. Examining our results, we will want to join on the state column in both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.merge(merged, areas, on='state', how='left')\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcf8cef",
   "metadata": {},
   "source": [
    "Again, let's check for nulls to see if there were any mismatches:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6512b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c9534",
   "metadata": {},
   "source": [
    "There are nulls in the area column; we can take a look to see which regions were ignored here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2ec411",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['state'][final['area (sq. mi)'].isnull()].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d577ea",
   "metadata": {},
   "source": [
    "We see that our areas DataFrame does not contain the area of the United States as a whole. We could insert the appropriate value (using the sum of all state areas, for instance), but in this case we'll just drop the null values because the population density of the entire United States is not relevant to our current discussion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22775a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.dropna(inplace=True)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86c015",
   "metadata": {},
   "source": [
    "Now we have all the data we need. To answer the question of interest, let's first select the portion of the data corresponding with the year 2000, and the total population. We'll use the query() function to do this quickly (this requires the numexpr package to be installed; see High-Performance Pandas: eval() and query()):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2010 = final.query(\"year == 2010 & ages == 'total'\")\n",
    "data2010.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888fef8a",
   "metadata": {},
   "source": [
    "Now let's compute the population density and display it in order. We'll start by re-indexing our data on the state, and then compute the result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbec7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2010.set_index('state', inplace=True)\n",
    "density = data2010['population'] / data2010['area (sq. mi)']\n",
    "\n",
    "density.sort_values(ascending=False, inplace=True)\n",
    "density.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856101a",
   "metadata": {},
   "source": [
    "The result is a ranking of US states plus Washington, DC, and Puerto Rico in order of their 2010 population density, in residents per square mile. We can see that by far the densest region in this dataset is Washington, DC (i.e., the District of Columbia); among states, the densest is New Jersey.\n",
    "\n",
    "We can also check the end of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "density.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad933bd",
   "metadata": {},
   "source": [
    "We see that the least dense state, by far, is Alaska, averaging slightly over one resident per square mile.\n",
    "\n",
    "This type of messy data merging is a common task when trying to answer questions using real-world data sources. I hope that this example has given you an idea of the ways you can combine tools we've covered in order to gain insight from your data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b51e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
